{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7e0dad4",
      "metadata": {},
      "source": [
        "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
        "\n",
        "**Format:** Structured interview with whiteboarding/documentation  \n",
        "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
        "\n",
        "**Please Fill in your Responses in the Response markdown boxes**\n",
        "\n",
        "---\n",
        "\n",
        "## Challenge Scenario\n",
        "\n",
        "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
        "\n",
        "**Current Pain Points:**\n",
        "- Manual code reviews take 2-3 days per PR\n",
        "- Inconsistent review quality across teams\n",
        "- Deployment failures due to missed edge cases\n",
        "- Security vulnerabilities slip through reviews\n",
        "- No standardized deployment process across projects\n",
        "- Rollback decisions are manual and slow\n",
        "\n",
        "**Business Requirements:**\n",
        "- Reduce review time to <4 hours for standard PRs\n",
        "- Maintain or improve code quality\n",
        "- Catch 90%+ of security vulnerabilities before deployment\n",
        "- Standardize deployment across 50+ microservices\n",
        "- Enable automatic rollback based on metrics\n",
        "- Support multiple environments (dev, staging, prod)\n",
        "- Handle both new features and hotfixes\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be761411",
      "metadata": {},
      "source": [
        "## Part A: Problem Decomposition (25 points)\n",
        "\n",
        "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
        "- Clear input requirements\n",
        "- Specific output format\n",
        "- Success criteria\n",
        "- Failure handling strategy\n",
        "\n",
        "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
        "\n",
        "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0a3c10",
      "metadata": {},
      "source": [
        "## Response Part A:\n",
        "### Step Breakdown\n",
        "1. **PR Intake & Context Aggregation**\n",
        "   - Inputs: repository URL, PR number, branch metadata, service ownership map, related incidents.\n",
        "   - Output: normalized PR package (code diff, file taxonomy, ownership, historical signals).\n",
        "   - Success: package stored in queue with completeness checksum and schema validation.\n",
        "   - Failure: if metadata missing, trigger retry with backoff and notify submitter via webhook.\n",
        "2. **Automated Baseline Analysis**\n",
        "   - Inputs: PR package, lint/test configuration templates, threat model catalog.\n",
        "   - Output: structured findings set (lint violations, dependency deltas, test matrix plan, initial risk hints).\n",
        "   - Success: analyzers finish within SLA and publish results to findings store.\n",
        "   - Failure: mark analyzer status, auto-ticket to platform team, fall back to cached configs.\n",
        "3. **AI Code Review Generation**\n",
        "   - Inputs: PR package, baseline findings, coding standards profile, reviewer notes.\n",
        "   - Output: AI review report (issue list with severity, rationale, suggested fixes, confidence scores).\n",
        "   - Success: report passes schema checks and contains at least one conclusion item.\n",
        "   - Failure: rerun with reduced diff chunks; if still failing, escalate to human reviewer queue.\n",
        "4. **Security & Compliance Evaluation**\n",
        "   - Inputs: AI review report, security policy library, secret scanning outputs, SBOM deltas.\n",
        "   - Output: security posture summary (pass/block, vulnerabilities, policy references).\n",
        "   - Success: all high severity policies evaluated with explicit status.\n",
        "   - Failure: block deployment, file security incident, attach artifacts for triage.\n",
        "5. **Automated Test & Deployment Simulation**\n",
        "   - Inputs: approved review artifacts, test matrix plan, environment config (dev/staging), deployment manifests.\n",
        "   - Output: test run results, canary deployment metrics, rollback readiness checklist.\n",
        "   - Success: tests meet thresholds; simulated deploy hits health KPIs.\n",
        "   - Failure: auto-rollback, collect telemetry, update issue tracker with root cause hints.\n",
        "6. **Release Decision & Orchestration**\n",
        "   - Inputs: aggregated scorecard (review, security, tests), service SLOs, change calendar.\n",
        "   - Output: deployment decision (approve/hold), rollout plan, notification payloads.\n",
        "   - Success: decision logged with traceable evidence and approvals captured.\n",
        "   - Failure: hold release, notify stakeholders, schedule human CAB review.\n",
        "7. **Post-Deployment Monitoring & Learning**\n",
        "   - Inputs: production telemetry, user feedback, incident alerts, deployment decision metadata.\n",
        "   - Output: post-release report, anomaly detections, feedback to learning store.\n",
        "   - Success: monitoring runs for defined dwell period and no untriaged alerts remain.\n",
        "   - Failure: auto-initiate rollback, open incident with context bundle, page on-call.\n",
        "\n",
        "### Orchestration Characteristics\n",
        "- Parallelizable: Steps 2 and 3 can run concurrently after Step 1; Step 4 starts once security scans in Step 2 finish; Step 5 can launch when Steps 3 and 4 are green.\n",
        "- Blocking: Step 6 waits on completion of Steps 3-5; Step 7 depends on a successful or rolled-back deployment from Step 6.\n",
        "- Critical decision points: review approval in Step 3, security gate in Step 4, release go/no-go in Step 6.\n",
        "\n",
        "### Handoffs & Data Contracts\n",
        "- Between Steps 1->2/3: PR package JSON, repo auth tokens, change risk tags.\n",
        "- Steps 2->3: baseline findings schema with lint/test/security summaries.\n",
        "- Steps 3->4: AI review report referencing file paths and vulnerability hints.\n",
        "- Steps 4->5: security gate decision plus mitigation requirements.\n",
        "- Steps 5->6: deployment simulation metrics, rollback plan, unmet test cases.\n",
        "- Steps 6->7: release decision log, rollout timestamps, monitoring configuration overrides.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb38e9fa",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1fdc377",
      "metadata": {},
      "source": [
        "## Part B: AI Prompting Strategy (30 points)\n",
        "\n",
        "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
        "- System role/persona definition\n",
        "- Structured input format\n",
        "- Expected output format\n",
        "- Examples of good vs bad responses\n",
        "- Error handling instructions\n",
        "\n",
        "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
        "- **Code that uses obscure libraries or frameworks**\n",
        "- **Security reviews for code**\n",
        "- **Performance analysis of database queries**\n",
        "- **Legacy code modifications**\n",
        "\n",
        "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd049f5",
      "metadata": {},
      "source": [
        "## Response Part B:\n",
        "### Prompt Design for Consecutive Steps\n",
        "**Step 3: AI Code Review Generation**\n",
        "- System Persona: \"You are Reviewer-GPT, a senior polyglot software architect known for precise, evidence-backed feedback. You cite line numbers and align with the team's quality rubric.\"\n",
        "- Input Template:\n",
        "```json\n",
        "{\n",
        "  \"diff_chunks\": [\"<unified_diff>\"],\n",
        "  \"repo_context\": {\"language\": \"python\", \"frameworks\": [\"FastAPI\"], \"service\": \"payments\"},\n",
        "  \"baseline_findings\": [\"string\"],\n",
        "  \"coding_standards\": {\"error_handling\": \"must use typed exceptions\"}\n",
        "}\n",
        "```\n",
        "- Expected Output:\n",
        "```json\n",
        "{\n",
        "  \"issues\": [\n",
        "    {\n",
        "      \"file\": \"string\",\n",
        "      \"line\": 0,\n",
        "      \"severity\": \"critical|major|minor|nit\",\n",
        "      \"title\": \"string\",\n",
        "      \"analysis\": \"string\",\n",
        "      \"recommendation\": \"string\",\n",
        "      \"confidence\": 0.0\n",
        "    }\n",
        "  ],\n",
        "  \"summary\": \"string\",\n",
        "  \"approval_recommendation\": \"approve|revise|block\"\n",
        "}\n",
        "```\n",
        "- Good Response: references diff context, ties to standards, includes actionable fix.\n",
        "- Bad Response: vague language, no line numbers, contradicts policy, hallucinated files.\n",
        "- Error Handling: if diff exceeds token limit, request segmented input; on schema mismatch, emit `{\"error\":\"schema_violation\",\"details\":...}`.\n",
        "\n",
        "**Step 4: Security & Compliance Evaluation**\n",
        "- System Persona: \"You are SecOps-GPT, an AppSec lead specializing in threat modeling, compliance (SOC2, PCI), and secure coding for large microservice fleets.\"\n",
        "- Input Template:\n",
        "```json\n",
        "{\n",
        "  \"review_report\": {\"issues\": []},\n",
        "  \"dependency_changes\": [{\"package\": \"string\", \"version_from\": \"string\", \"version_to\": \"string\"}],\n",
        "  \"policy_library\": [{\"id\": \"POL-12\", \"requirement\": \"No hard-coded secrets\"}],\n",
        "  \"sbom_diff\": \"<cyclonedx_json>\",\n",
        "  \"runtime_flags\": {\"data_classification\": \"PII\"}\n",
        "}\n",
        "```\n",
        "- Expected Output:\n",
        "```json\n",
        "{\n",
        "  \"status\": \"pass|conditional|fail\",\n",
        "  \"vulnerabilities\": [\n",
        "    {\"id\": \"string\", \"cwe\": \"string\", \"evidence\": \"string\", \"mitigation\": \"string\"}\n",
        "  ],\n",
        "  \"policy_gaps\": [\"string\"],\n",
        "  \"next_steps\": [\"string\"],\n",
        "  \"confidence\": 0.0\n",
        "}\n",
        "```\n",
        "- Good Response: maps findings to policies/CWEs, flags compensating controls, quantifies risk.\n",
        "- Bad Response: blanket approvals without evidence, missing PCI requirements, ignores high CVSS scores.\n",
        "- Error Handling: if SBOM parse fails, respond with `status\":\"fail\"` and log `sbom_unreadable`; direct human follow-up trigger.\n",
        "\n",
        "### Prompt Hardening Strategies\n",
        "- Obscure Libraries: augment context with auto-generated API docs/snippets before invoking Reviewer-GPT; require the model to cite the source snippet used.\n",
        "- Security Reviews: prepend recent CVE feeds and mandate CWE tagging; enforce zero-trust default (fail unless explicit mitigation).\n",
        "- DB Performance: include query plans and dataset scale; ask for indexed recommendations and expected impact metrics.\n",
        "- Legacy Code: provide architectural decision records and change budget; require backward compatibility checklist in output.\n",
        "\n",
        "### Prompt Effectiveness\n",
        "- Run regression suites of historical PRs with known outcomes; diff AI reports vs ground truth.\n",
        "- Track output schema compliance and confidence calibration; auto-adjust temperature when variance spikes.\n",
        "- Embed feedback hooks so humans can rate suggestions; feed scores into prompt/parameter tuning pipeline.\n",
        "- Monitor drift via weekly evaluation sets covering multiple languages and edge cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476e98d3",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59d353d",
      "metadata": {},
      "source": [
        "## Part C: System Architecture & Reusability (25 points)\n",
        "\n",
        "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
        "- Configuration management\n",
        "- Language/framework variations\n",
        "- Different deployment targets (cloud providers, on-prem)\n",
        "- Team-specific coding standards\n",
        "- Industry-specific compliance requirements\n",
        "\n",
        "**Question 3.2:** How would the system get better over time based on:\n",
        "- False positive/negative rates in reviews\n",
        "- Deployment success/failure patterns\n",
        "- Developer feedback\n",
        "- Production incident correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0052f045",
      "metadata": {},
      "source": [
        "## Response Part C:\n",
        "### Reusability Strategy\n",
        "- **Configuration Layers:** separate global defaults, project overrides, and team-level profiles using declarative YAML; load via feature flags per microservice.\n",
        "- **Language/Framework Support:** use adapter interfaces for analyzers (e.g., lint, tests) so new language support only requires implementing the adapter contract and updating capability registry.\n",
        "- **Deployment Targets:** abstract environment definitions (Kubernetes, serverless, on-prem) behind Terraform/Helm modules selected via metadata; ensure secrets management integrates with cloud-specific vaults.\n",
        "- **Team Standards:** store linters, style guides, and review rubrics in versioned catalogs; let teams inherit and extend baselines while central policy enforces minimum bars.\n",
        "- **Compliance:** map controls (SOC2, HIPAA, PCI) to pipeline checks; tag services with data classification so the right compliance pack auto-attaches.\n",
        "\n",
        "### Continuous Improvement\n",
        "- Ingest false positive/negative signals from reviewer overrides and post-deployment incidents; retrain detection prompts/models with labeled data.\n",
        "- Analyze deployment failure trends to adjust risk scoring weights, add pre-deploy checks, or recommend runbook updates.\n",
        "- Collect developer feedback via inline review ratings and retro surveys; prioritize enhancements in backlog.\n",
        "- Correlate production incidents with preceding reviews to find missed patterns and update both prompts and static rules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6029f169",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d096eb",
      "metadata": {},
      "source": [
        "## Part D: Implementation Strategy (20 points)\n",
        "\n",
        "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
        "- MVP definition (what's the minimum viable system?)\n",
        "- Pilot program strategy\n",
        "- Rollout phases\n",
        "- Success metrics for each phase\n",
        "\n",
        "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
        "- AI making incorrect review decisions\n",
        "- System downtime during critical deployments\n",
        "- Integration failures with existing tools\n",
        "- Resistance from development teams\n",
        "- Compliance/audit requirements\n",
        "\n",
        "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
        "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
        "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
        "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
        "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
        "- Communication tools (Slack, Teams, Jira)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fa9820",
      "metadata": {},
      "source": [
        "## Response Part D:\n",
        "### 6-Month Roadmap\n",
        "- **Month 0-1 (MVP):** deliver PR intake, baseline analysis, and AI code review for one language/service; success = 80% of PRs get AI review within 2 hours, human reviewers accept >60% of suggestions.\n",
        "- **Month 2 (Pilot):** add security evaluation and staging deployment simulation for 3 services; success = reduce pilot PR cycle time by 40%, zero missed critical vulnerabilities.\n",
        "- **Month 3-4 (Expansion):** integrate automated rollout decision, add Slack/Jira notifications, support multi-language adapters; success = 70% automated approvals, <5% rollback rate in staging.\n",
        "- **Month 5 (Production Rollout):** extend to 20 services, enable production canary gates, instrument monitoring dashboards; success = median review-to-deploy <6 hours, automated rollback executes within 5 minutes when triggered.\n",
        "- **Month 6 (Scale & Optimize):** onboard remaining microservices, enforce compliance packs, finalize self-serve configuration UI; success = company-wide standardized pipeline, audit-ready evidence exports.\n",
        "\n",
        "### Risk Mitigation\n",
        "- **Incorrect AI Reviews:** require human confirmation for high-severity findings initially; maintain reviewer confidence thresholds and provide quick \"disagree\" workflows feeding model retraining.\n",
        "- **Deployment Downtime:** use blue/green or canary strategies with automatic rollback; rehearse incident runbooks monthly.\n",
        "- **Integration Failures:** build sandbox connectors first, include contract tests against GitHub/GitLab/Jenkins APIs, add fallback to manual triggers.\n",
        "- **Team Resistance:** deliver transparent metrics, optional opt-in pilot, training sessions, and highlight time savings; allow human override at every gate.\n",
        "- **Compliance/Audit:** log every decision with immutable storage (e.g., AWS QLDB), generate signed reports, keep manual approval path for regulated releases.\n",
        "\n",
        "### Tool Choices\n",
        "- Code Review: GitHub PR APIs + GraphQL for metadata, extend with GitHub Checks.\n",
        "- CI/CD: Harness existing Jenkins/GitHub Actions pipelines using reusable workflow templates.\n",
        "- Monitoring: Datadog for deployment health, Prometheus for SLO validation, integrate PagerDuty for alerts.\n",
        "- Security: Snyk for dependency scanning, SonarQube + Semgrep for static analysis, Trivy for container images.\n",
        "- Communication: Slack for notifications, Jira for ticketing, Confluence for runbooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584added",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}